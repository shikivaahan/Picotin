{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2dd8a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "db7cd953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define food and drinks sets\n",
    "food = {\n",
    "    'Add On', 'Bar Snack', 'Bites', 'Brunch', 'Burger', 'Chargrill', 'Chicken or Prawn Add ',\n",
    "    'Combo', 'Dessert', 'Doneness', 'Foods', 'GF Add Ons', 'GF Burger and Hotdog', 'Mains',\n",
    "    'Pasta', 'Pizza', 'Platter', 'Serving Choice', 'Sides', 'Smoothie', 'Soup', 'Starter', 'Hotdog'\n",
    "}\n",
    "\n",
    "drinks = {\n",
    "    'Bottled Beer', 'Bourbon', 'Choice of Margarita / ', 'Ciders', 'Cocktail', 'Coffee',\n",
    "    'Coffee Options', 'Cognac', 'Corkage', 'Draught Beers', 'Drinks', 'Gin', 'Juices',\n",
    "    'Liqueurs & Aperitifs', 'Live Craft Beer', 'Mineral Water', 'Mocktails', 'Red Wine',\n",
    "    'Rose Wine', 'Rum', 'Soft Drink', 'Sparkling & Champagne', 'Tea', 'Tequila', 'Vodka',\n",
    "    'Whisky', 'White Wine'\n",
    "}\n",
    "\n",
    "# Date range boundaries\n",
    "start_date = pd.Timestamp('2024-12-01')\n",
    "end_date = pd.Timestamp('2025-03-31')\n",
    "\n",
    "def clean_itemgroup_data(file_path):\n",
    "    # Read Excel file\n",
    "    df = pd.read_excel(file_path, engine='xlrd', skiprows=4)\n",
    "    df = df[:-1]\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    df.columns = ['Date'] + list(df.columns[1:])\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    if 'Unnamed: 2' in df.columns:\n",
    "        df = df.drop(columns=['Unnamed: 2'])\n",
    "    \n",
    "    df = df[df['Date'] != 'Total']\n",
    "    df = df[df['Date'].notna()]\n",
    "    \n",
    "    # Drop columns with 'Unnamed' prefix\n",
    "    unnamed_columns = [col for col in df.columns if col.startswith('Unnamed')]\n",
    "    df = df.drop(columns=unnamed_columns)\n",
    "    \n",
    "    # Format the Date column as Timestamp\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%d-%b-%y')\n",
    "    \n",
    "    # Filter by date range (using Timestamps, not string comparison)\n",
    "    df = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]\n",
    "    \n",
    "    # (Removed the line converting Date back to string to keep it as Timestamp)\n",
    "    \n",
    "    # Convert all columns except 'Date' to float, handling non-numeric values\n",
    "    for col in df.columns[1:]:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Calculate total_food and total_drinks for each row\n",
    "    food_cols = list(food.intersection(df.columns))\n",
    "    drink_cols = list(drinks.intersection(df.columns))\n",
    "\n",
    "    df['total_food'] = df[food_cols].sum(axis=1, skipna=True)\n",
    "    df['total_drinks'] = df[drink_cols].sum(axis=1, skipna=True)\n",
    "    \n",
    "    # Drop all columns except Date, total_food, total_drinks, Total\n",
    "    columns_to_keep = ['Date', 'total_food', 'total_drinks', 'Total']\n",
    "    df = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "    \n",
    "    # Convert column names to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_daily_summary(file_path):\n",
    "    # Read CSV file with appropriate skipping of rows and footers\n",
    "    df = pd.read_csv(file_path, skiprows=4, skipfooter=7, engine='python')\n",
    "    \n",
    "    # Drop unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if col.startswith('Unnamed:')]\n",
    "    df.drop(columns=unnamed_columns, inplace=True)\n",
    "    \n",
    "    # Convert START column to date (Timestamp) format\n",
    "    df['START'] = pd.to_datetime(df['START'], format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "    \n",
    "    # Filter by date range (Timestamp comparison)\n",
    "    df = df[(df['START'] >= start_date) & (df['START'] <= end_date)]\n",
    "    \n",
    "    # (Removed the line converting START to string to keep it as Timestamp)\n",
    "    \n",
    "    # Drop all other columns except START, ORDERS COUNT, AVG ORDER\n",
    "    columns_to_keep = ['START', 'ORDERS COUNT', 'AVG ORDER']\n",
    "    df = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "    \n",
    "    # Rename START to date\n",
    "    df.rename(columns={'START': 'date'}, inplace=True)\n",
    "    \n",
    "    # Convert all column names to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    # Group by date and sum the values for duplicates\n",
    "    df = df.groupby('date', as_index=False).sum()\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_outlet(itemgroup, daily_summary, outlet_name):\n",
    "    # Merge itemgroup and daily summary using an outer join on date\n",
    "    merged = pd.merge(itemgroup, daily_summary, on='date', how='outer', suffixes=('_itemgroup', '_daily'))\n",
    "    # Sort by date\n",
    "    merged = merged.sort_values(by='date').reset_index(drop=True)\n",
    "    # Rename the merged DataFrame for each outlet\n",
    "    merged.name = f\"{outlet_name}_merged\"\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0afc340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'daily_data\\Picotin_overview.xlsx', engine='openpyxl')\n",
    "df = df.drop(columns=['Timestamp'])\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# Manually replace incorrect entries\n",
    "df.loc[167, 'Date'] = '2024-12-11 00:00:00'\n",
    "df.loc[170, 'Date'] = '2024-12-12 00:00:00'\n",
    "df.loc[456, 'Date'] = '2025-04-01 00:00:00'\n",
    "df.loc[459, 'Date'] = '2025-04-02 00:00:00'\n",
    "df.loc[461, 'Date'] = '2025-04-03 00:00:00'\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='raise')\n",
    "\n",
    "# Removed string formatting so we keep Timestamps\n",
    "# df['Date'] = df['Date'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "# Filter with actual Timestamps\n",
    "df = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)].copy()\n",
    "\n",
    "ASQ_itemgroup = clean_itemgroup_data(r'daily_data\\ASQ_itemgroup_daily.xls')\n",
    "ASQ_daily_summary = clean_daily_summary(r'daily_data\\ASQ_daily_summary.csv')\n",
    "\n",
    "KATONG_itemgroup = clean_itemgroup_data(r'daily_data\\KATONG_itemgroup_daily.xls')\n",
    "KATONG_daily_summary = clean_daily_summary(r'daily_data\\KATONG_daily_summary.csv')\n",
    "\n",
    "RC_itemgroup = clean_itemgroup_data(r'daily_data\\RC_itemgroup_daily.xls')\n",
    "RC_daily_summary = clean_daily_summary(r'daily_data\\RC_daily_summary.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ccfc2b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outlet: ASQ\n",
      "Dates missing in itemgroup: []\n",
      "Dates missing in daily summary: []\n",
      "Duplicate dates in daily summary: []\n",
      "Duplicate dates in itemgroup: []\n",
      "\n",
      "Outlet: KATONG\n",
      "Dates missing in itemgroup: []\n",
      "Dates missing in daily summary: [datetime.date(2025, 3, 31)]\n",
      "Duplicate dates in daily summary: []\n",
      "Duplicate dates in itemgroup: []\n",
      "\n",
      "Outlet: RC\n",
      "Dates missing in itemgroup: []\n",
      "Dates missing in daily summary: [datetime.date(2024, 12, 1), datetime.date(2024, 12, 5), datetime.date(2024, 12, 24), datetime.date(2025, 1, 18), datetime.date(2025, 3, 31)]\n",
      "Duplicate dates in daily summary: [datetime.date(2024, 12, 4) datetime.date(2024, 12, 23)\n",
      " datetime.date(2025, 1, 17)]\n",
      "Duplicate dates in itemgroup: []\n"
     ]
    }
   ],
   "source": [
    "outlets = ['ASQ', 'KATONG', 'RC']\n",
    "\n",
    "for outlet in outlets:\n",
    "    # Access the respective DataFrames dynamically\n",
    "    daily_summary = globals().get(f'{outlet}_daily_summary')\n",
    "    itemgroup = globals().get(f'{outlet}_itemgroup')\n",
    "    \n",
    "    if daily_summary is None or itemgroup is None:\n",
    "        print(f\"\\nWarning: DataFrames for {outlet} are not loaded.\")\n",
    "        continue\n",
    "\n",
    "    # Convert Timestamps to *date* only (ignore time)\n",
    "    summary_date_only = daily_summary['date'].dropna().dt.date\n",
    "    itemgroup_date_only = itemgroup['date'].dropna().dt.date\n",
    "\n",
    "    # Find unique dates (no time)\n",
    "    summary_dates = set(summary_date_only.unique())\n",
    "    itemgroup_dates = set(itemgroup_date_only.unique())\n",
    "\n",
    "    # Find missing dates, ignoring time\n",
    "    missing_in_itemgroup = summary_dates - itemgroup_dates\n",
    "    missing_in_summary   = itemgroup_dates - summary_dates\n",
    "\n",
    "    # Find duplicate dates, ignoring time\n",
    "    duplicates_in_summary    = summary_date_only[summary_date_only.duplicated()]\n",
    "    duplicates_in_itemgroup  = itemgroup_date_only[itemgroup_date_only.duplicated()]\n",
    "\n",
    "    # Display the results for the current outlet\n",
    "    print(f\"\\nOutlet: {outlet}\")\n",
    "    print(\"Dates missing in itemgroup:\", sorted(missing_in_itemgroup))\n",
    "    print(\"Dates missing in daily summary:\", sorted(missing_in_summary))\n",
    "    print(\"Duplicate dates in daily summary:\", duplicates_in_summary.unique())\n",
    "    print(\"Duplicate dates in itemgroup:\", duplicates_in_itemgroup.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "75a73fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASQ_merged = merge_outlet(ASQ_itemgroup, ASQ_daily_summary, 'ASQ')\n",
    "KATONG_merged = merge_outlet(KATONG_itemgroup, KATONG_daily_summary, 'KATONG')\n",
    "RC_merged = merge_outlet(RC_itemgroup, RC_daily_summary, 'RC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8d5591e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of 'Date' in df:\n",
      "datetime64[ns]\n",
      "\n",
      "Data type of 'date' in ASQ_merged:\n",
      "datetime64[ns]\n",
      "\n",
      "Number of empty or NaN values in 'Date' column of df:\n",
      "0\n",
      "\n",
      "Number of empty or NaN values in 'date' column of ASQ_merged:\n",
      "0\n",
      "\n",
      "Rows with empty 'Date' in df:\n",
      "Empty DataFrame\n",
      "Columns: [Outlet, Date, Day, Breakfast Sales, Lunch Sales, Evening Sales, Dinner Lunch Sales, Night Sales, Total Sales, Number of Breakfast Floor Staff, Number of Lunch Floor Staff, Number of Evening Floor Staff, Number of Dinner Floor Staff, Number of Night Floor Staff, Number of Breakfast Kitchen Staff, Number of Lunch Kitchen Staff, Number of Evening Kitchen Staff, Number of Dinner Kitchen Staff, Number of Night Kitchen Staff, Total Taxi Claims, Last Bill Closed, Closing Time, Closing Manager Name, Total Floor Staff Hours, Total Kitchen Staff Hours]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 25 columns]\n",
      "\n",
      "Rows with empty 'date' in ASQ_merged:\n",
      "Empty DataFrame\n",
      "Columns: [date, total_food, total_drinks, total, orders count, avg order]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df_copy = df.copy()\n",
    "\n",
    "# Print data types of both date columns\n",
    "print(\"Data type of 'Date' in df:\")\n",
    "print(df['Date'].dtype)\n",
    "print(\"\\nData type of 'date' in ASQ_merged:\")\n",
    "print(ASQ_merged['date'].dtype)\n",
    "\n",
    "# Check for empty or null values in both date columns\n",
    "print(\"\\nNumber of empty or NaN values in 'Date' column of df:\")\n",
    "print(df['Date'].isna().sum())\n",
    "\n",
    "print(\"\\nNumber of empty or NaN values in 'date' column of ASQ_merged:\")\n",
    "print(ASQ_merged['date'].isna().sum())\n",
    "\n",
    "# Print rows where Date is NaN in both dataframes\n",
    "print(\"\\nRows with empty 'Date' in df:\")\n",
    "print(df[df['Date'].isna()])\n",
    "\n",
    "print(\"\\nRows with empty 'date' in ASQ_merged:\")\n",
    "print(ASQ_merged[ASQ_merged['date'].isna()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fa96e22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152   2024-12-06\n",
      "162   2024-12-09\n",
      "165   2024-12-10\n",
      "167   2024-12-11\n",
      "171   2024-12-12\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "0   2024-12-02 00:00:00\n",
      "1   2024-12-02 09:29:03\n",
      "2   2024-12-03 00:00:00\n",
      "3   2024-12-03 10:43:14\n",
      "4   2024-12-04 00:00:00\n",
      "Name: date, dtype: datetime64[ns]\n",
      "Number of mismatched dates: 6\n",
      "Date 2024-12-03 is in ASQ_merged but not in df_x\n",
      "Date 2024-12-02 is in ASQ_merged but not in df_x\n",
      "Date 2024-12-31 is in ASQ_merged but not in df_x\n",
      "Date 2024-12-05 is in ASQ_merged but not in df_x\n",
      "Date 2024-12-04 is in ASQ_merged but not in df_x\n",
      "Date 2024-12-17 is in ASQ_merged but not in df_x\n"
     ]
    }
   ],
   "source": [
    "df_x = df[df['Outlet'] == 'Asia Square'].copy()\n",
    "\n",
    "# Keep them as Timestamps\n",
    "df_x['Date'] = pd.to_datetime(df_x['Date'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "ASQ_merged['date'] = pd.to_datetime(ASQ_merged['date'], errors='coerce')\n",
    "\n",
    "# Print the first few rows (still Timestamps with potential times)\n",
    "print(df_x['Date'].head())\n",
    "print(ASQ_merged['date'].head())\n",
    "\n",
    "# Convert to date-only for comparison\n",
    "df_x_date_only = df_x['Date'].dropna().dt.date\n",
    "ASQ_merged_date_only = ASQ_merged['date'].dropna().dt.date\n",
    "\n",
    "# Build sets of unique date-only values\n",
    "df_x_dates = set(df_x_date_only.unique())\n",
    "ASQ_merged_dates = set(ASQ_merged_date_only.unique())\n",
    "\n",
    "# Find dates that are in one DataFrame but not the other (ignoring time)\n",
    "mismatched_dates = df_x_dates.symmetric_difference(ASQ_merged_dates)\n",
    "\n",
    "print(f\"Number of mismatched dates: {len(mismatched_dates)}\")\n",
    "\n",
    "# Report each mismatched date, specifying where it is missing\n",
    "for date in mismatched_dates:\n",
    "    if date in df_x_dates:\n",
    "        print(f\"Date {date} is in df_x but not in ASQ_merged\")\n",
    "    else:\n",
    "        print(f\"Date {date} is in ASQ_merged but not in df_x\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
