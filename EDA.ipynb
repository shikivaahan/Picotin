{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dd8a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db7cd953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define food and drinks sets\n",
    "food = {'Add On', 'Bar Snack', 'Bites', 'Brunch', 'Burger', 'Chargrill', 'Chicken or Prawn Add ', \n",
    "        'Combo', 'Dessert', 'Doneness', 'Foods', 'GF Add Ons', 'GF Burger and Hotdog', 'Mains', \n",
    "        'Pasta', 'Pizza', 'Platter', 'Serving Choice', 'Sides', 'Smoothie', 'Soup', 'Starter', 'Hotdog'}\n",
    "\n",
    "drinks = {'Bottled Beer', 'Bourbon', 'Choice of Margarita / ', 'Ciders', 'Cocktail', 'Coffee', \n",
    "          'Coffee Options', 'Cognac', 'Corkage', 'Draught Beers', 'Drinks', 'Gin', 'Juices', \n",
    "          'Liqueurs & Aperitifs', 'Live Craft Beer', 'Mineral Water', 'Mocktails', 'Red Wine', \n",
    "          'Rose Wine', 'Rum', 'Soft Drink', 'Sparkling & Champagne', 'Tea', 'Tequila', 'Vodka', \n",
    "          'Whisky', 'White Wine'}\n",
    "\n",
    "# Date range boundaries\n",
    "start_date = pd.Timestamp('2024-12-01')\n",
    "end_date = pd.Timestamp('2025-03-31')\n",
    "\n",
    "def clean_itemgroup_data(file_path):\n",
    "    # Read Excel file\n",
    "    df = pd.read_excel(file_path, engine='xlrd', skiprows=4)\n",
    "    df = df[:-1]\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    df.columns = ['Date'] + list(df.columns[1:])\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    if 'Unnamed: 2' in df.columns:\n",
    "        df = df.drop(columns=['Unnamed: 2'])\n",
    "    \n",
    "    df = df[df['Date'] != 'Total']\n",
    "    df = df[df['Date'].notna()]\n",
    "    \n",
    "    # Drop columns with 'Unnamed' prefix\n",
    "    unnamed_columns = [col for col in df.columns if col.startswith('Unnamed')]\n",
    "    df = df.drop(columns=unnamed_columns)\n",
    "    \n",
    "    # Format the Date column\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%d-%b-%y')\n",
    "    \n",
    "    # Filter by date range\n",
    "    df = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]\n",
    "    \n",
    "    # Convert the date back to the desired format\n",
    "    df['Date'] = df['Date'].dt.strftime('%d/%m/%Y')\n",
    "    \n",
    "    # Convert all columns except 'Date' to float, handling non-numeric values\n",
    "    for col in df.columns[1:]:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Calculate total_food and total_drinks for each row\n",
    "    food_cols = list(food.intersection(df.columns))\n",
    "    drink_cols = list(drinks.intersection(df.columns))\n",
    "\n",
    "    df['total_food'] = df[food_cols].sum(axis=1, skipna=True)\n",
    "    df['total_drinks'] = df[drink_cols].sum(axis=1, skipna=True)\n",
    "    \n",
    "    # Drop all columns except Date, total_food, total_drinks, Total\n",
    "    columns_to_keep = ['Date', 'total_food', 'total_drinks', 'Total']\n",
    "    df = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "    \n",
    "    # Convert column names to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_daily_summary(file_path):\n",
    "    # Read CSV file with appropriate skipping of rows and footers\n",
    "    df = pd.read_csv(file_path, skiprows=4, skipfooter=7, engine='python')\n",
    "    \n",
    "    # Drop unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if col.startswith('Unnamed:')]\n",
    "    df.drop(columns=unnamed_columns, inplace=True)\n",
    "    \n",
    "    # Convert START column to date format\n",
    "    df['START'] = pd.to_datetime(df['START'], format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "    \n",
    "    # Filter by date range\n",
    "    df = df[(df['START'] >= start_date) & (df['START'] <= end_date)]\n",
    "    \n",
    "    # Format the date to the desired format\n",
    "    df['START'] = df['START'].dt.strftime('%d/%m/%Y')\n",
    "    \n",
    "    # Drop all other columns except START, ORDERS COUNT, AVG ORDER\n",
    "    columns_to_keep = ['START', 'ORDERS COUNT', 'AVG ORDER']\n",
    "    df = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "    \n",
    "    # Rename START to date\n",
    "    df.rename(columns={'START': 'date'}, inplace=True)\n",
    "    \n",
    "    # Convert all column names to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    # Group by date and sum the values for duplicates\n",
    "    df = df.groupby('date', as_index=False).sum()\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_outlet(itemgroup, daily_summary, outlet_name):\n",
    "    # Merge itemgroup and daily summary using an outer join on date\n",
    "    merged = pd.merge(itemgroup, daily_summary, on='date', how='outer', suffixes=('_itemgroup', '_daily'))\n",
    "    # Sort by date\n",
    "    merged = merged.sort_values(by='date').reset_index(drop=True)\n",
    "    # Rename the merged DataFrame for each outlet\n",
    "    merged.name = f\"{outlet_name}_merged\"\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afc340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'daily_data\\Picotin_overview.xlsx', engine='openpyxl')\n",
    "df = df.drop(columns=['Timestamp'])\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "#manually replace incorrect entries\n",
    "df.loc[167, 'Date'] = '2024-12-11 00:00:00'\n",
    "df.loc[170, 'Date'] = '2024-12-12 00:00:00'\n",
    "df.loc[456, 'Date'] = '2025-04-01 00:00:00'\n",
    "df.loc[459, 'Date'] = '2025-04-02 00:00:00'\n",
    "df.loc[461, 'Date'] = '2025-04-03 00:00:00'\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='raise')\n",
    "\n",
    "# Format the final date column to the desired format: MM/DD/YYYY\n",
    "df['Date'] = df['Date'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "df = df[(df['Date'] >= start_date.strftime('%d/%m/%Y')) & \n",
    "    (df['Date'] <= end_date.strftime('%d/%m/%Y'))].copy()\n",
    "\n",
    "ASQ_itemgroup = clean_itemgroup_data(r'daily_data\\ASQ_itemgroup_daily.xls')\n",
    "ASQ_daily_summary = clean_daily_summary(r'daily_data\\ASQ_daily_summary.csv')\n",
    "\n",
    "KATONG_itemgroup = clean_itemgroup_data(r'daily_data\\KATONG_itemgroup_daily.xls')\n",
    "KATONG_daily_summary = clean_daily_summary(r'daily_data\\KATONG_daily_summary.csv')\n",
    "\n",
    "RC_itemgroup = clean_itemgroup_data(r'daily_data\\RC_itemgroup_daily.xls')\n",
    "RC_daily_summary = clean_daily_summary(r'daily_data\\RC_daily_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccfc2b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outlet: ASQ\n",
      "Dates missing in itemgroup: []\n",
      "Dates missing in daily summary: []\n",
      "Duplicate dates in daily summary: []\n",
      "Duplicate dates in itemgroup: []\n",
      "\n",
      "Outlet: KATONG\n",
      "Dates missing in itemgroup: []\n",
      "Dates missing in daily summary: ['31/03/2025']\n",
      "Duplicate dates in daily summary: []\n",
      "Duplicate dates in itemgroup: []\n",
      "\n",
      "Outlet: RC\n",
      "Dates missing in itemgroup: []\n",
      "Dates missing in daily summary: ['01/12/2024', '05/12/2024', '18/01/2025', '24/12/2024', '31/03/2025']\n",
      "Duplicate dates in daily summary: []\n",
      "Duplicate dates in itemgroup: []\n"
     ]
    }
   ],
   "source": [
    "outlets = ['ASQ', 'KATONG', 'RC']\n",
    "\n",
    "for outlet in outlets:\n",
    "    # Access the respective DataFrames dynamically\n",
    "    daily_summary = globals().get(f'{outlet}_daily_summary')\n",
    "    itemgroup = globals().get(f'{outlet}_itemgroup')\n",
    "    \n",
    "    if daily_summary is None or itemgroup is None:\n",
    "        print(f\"\\nWarning: DataFrames for {outlet} are not loaded.\")\n",
    "        continue\n",
    "\n",
    "    # Find unique dates from both DataFrames\n",
    "    summary_dates = set(daily_summary['date'].dropna().unique())\n",
    "    itemgroup_dates = set(itemgroup['date'].dropna().unique())\n",
    "\n",
    "    # Find missing dates\n",
    "    missing_in_itemgroup = summary_dates - itemgroup_dates\n",
    "    missing_in_summary = itemgroup_dates - summary_dates\n",
    "\n",
    "    # Find duplicate dates in each DataFrame\n",
    "    duplicates_in_summary = daily_summary['date'][daily_summary['date'].duplicated()]\n",
    "    duplicates_in_itemgroup = itemgroup['date'][itemgroup['date'].duplicated()]\n",
    "\n",
    "    # Display the results for the current outlet\n",
    "    print(f\"\\nOutlet: {outlet}\")\n",
    "    print(\"Dates missing in itemgroup:\", sorted(missing_in_itemgroup))\n",
    "    print(\"Dates missing in daily summary:\", sorted(missing_in_summary))\n",
    "    print(\"Duplicate dates in daily summary:\", duplicates_in_summary.unique())\n",
    "    print(\"Duplicate dates in itemgroup:\", duplicates_in_itemgroup.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75a73fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge each outlet\n",
    "ASQ_merged = merge_outlet(ASQ_itemgroup, ASQ_daily_summary, 'ASQ')\n",
    "KATONG_merged = merge_outlet(KATONG_itemgroup, KATONG_daily_summary, 'KATONG')\n",
    "RC_merged = merge_outlet(RC_itemgroup, RC_daily_summary, 'RC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d5591e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of 'Date' in df:\n",
      "object\n",
      "\n",
      "Data type of 'date' in ASQ_merged:\n",
      "object\n",
      "\n",
      "Number of empty or NaN values in 'Date' column of df:\n",
      "0\n",
      "\n",
      "Number of empty or NaN values in 'date' column of ASQ_merged:\n",
      "0\n",
      "\n",
      "Rows with empty 'Date' in df:\n",
      "Empty DataFrame\n",
      "Columns: [Outlet, Date, Day, Breakfast Sales, Lunch Sales, Evening Sales, Dinner Lunch Sales, Night Sales, Total Sales, Number of Breakfast Floor Staff, Number of Lunch Floor Staff, Number of Evening Floor Staff, Number of Dinner Floor Staff, Number of Night Floor Staff, Number of Breakfast Kitchen Staff, Number of Lunch Kitchen Staff, Number of Evening Kitchen Staff, Number of Dinner Kitchen Staff, Number of Night Kitchen Staff, Total Taxi Claims, Last Bill Closed, Closing Time, Closing Manager Name, Total Floor Staff Hours, Total Kitchen Staff Hours]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 25 columns]\n",
      "\n",
      "Rows with empty 'date' in ASQ_merged:\n",
      "Empty DataFrame\n",
      "Columns: [date, total_food, total_drinks, total, orders count, avg order]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df_copy = df.copy()\n",
    "\n",
    "\n",
    "# Print data types of both date columns\n",
    "print(\"Data type of 'Date' in df:\")\n",
    "print(df['Date'].dtype)\n",
    "print(\"\\nData type of 'date' in ASQ_merged:\")\n",
    "print(ASQ_merged['date'].dtype)\n",
    "\n",
    "# Check for empty or null values in both date columns\n",
    "print(\"\\nNumber of empty or NaN values in 'Date' column of df:\")\n",
    "print(df['Date'].isna().sum())\n",
    "\n",
    "print(\"\\nNumber of empty or NaN values in 'date' column of ASQ_merged:\")\n",
    "print(ASQ_merged['date'].isna().sum())\n",
    "\n",
    "# Print rows where Date is NaN in both dataframes\n",
    "print(\"\\nRows with empty 'Date' in df:\")\n",
    "print(df[df['Date'].isna()])\n",
    "\n",
    "print(\"\\nRows with empty 'date' in ASQ_merged:\")\n",
    "print(ASQ_merged[ASQ_merged['date'].isna()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa96e22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    04/11/2024\n",
      "2    05/11/2024\n",
      "3    06/11/2024\n",
      "4    07/11/2024\n",
      "5    08/11/2024\n",
      "Name: Date, dtype: object\n",
      "0    02/01/2025\n",
      "1    02/12/2024\n",
      "2    03/01/2025\n",
      "3    03/02/2025\n",
      "4    03/03/2025\n",
      "Name: date, dtype: object\n",
      "Number of mismatched dates: 52\n",
      "Date 19/11/2024 is in df_x but not in ASQ_merged\n",
      "Date 29/10/2024 is in df_x but not in ASQ_merged\n",
      "Date 31/12/2024 is in ASQ_merged but not in df_x\n",
      "Date 02/12/2024 is in ASQ_merged but not in df_x\n",
      "Date 14/10/2024 is in df_x but not in ASQ_merged\n",
      "Date 07/04/2025 is in df_x but not in ASQ_merged\n",
      "Date 02/10/2024 is in df_x but not in ASQ_merged\n",
      "Date 14/11/2024 is in df_x but not in ASQ_merged\n",
      "Date 11/10/2024 is in df_x but not in ASQ_merged\n",
      "Date 04/11/2024 is in df_x but not in ASQ_merged\n",
      "Date 03/10/2024 is in df_x but not in ASQ_merged\n",
      "Date 04/12/2024 is in ASQ_merged but not in df_x\n",
      "Date 08/04/2025 is in df_x but not in ASQ_merged\n",
      "Date 24/10/2024 is in df_x but not in ASQ_merged\n",
      "Date 08/11/2024 is in df_x but not in ASQ_merged\n",
      "Date 08/10/2024 is in df_x but not in ASQ_merged\n",
      "Date 18/11/2024 is in df_x but not in ASQ_merged\n",
      "Date 02/04/2025 is in df_x but not in ASQ_merged\n",
      "Date 16/10/2024 is in df_x but not in ASQ_merged\n",
      "Date 28/10/2024 is in df_x but not in ASQ_merged\n",
      "Date 27/11/2024 is in df_x but not in ASQ_merged\n",
      "Date 06/11/2024 is in df_x but not in ASQ_merged\n",
      "Date 23/10/2024 is in df_x but not in ASQ_merged\n",
      "Date 20/11/2024 is in df_x but not in ASQ_merged\n",
      "Date 07/11/2024 is in df_x but not in ASQ_merged\n",
      "Date 03/04/2025 is in df_x but not in ASQ_merged\n",
      "Date 13/11/2024 is in df_x but not in ASQ_merged\n",
      "Date 15/11/2024 is in df_x but not in ASQ_merged\n",
      "Date 21/11/2024 is in df_x but not in ASQ_merged\n",
      "Date 18/10/2024 is in df_x but not in ASQ_merged\n",
      "Date 22/11/2024 is in df_x but not in ASQ_merged\n",
      "Date 28/11/2024 is in df_x but not in ASQ_merged\n",
      "Date 12/11/2024 is in df_x but not in ASQ_merged\n",
      "Date 26/11/2024 is in df_x but not in ASQ_merged\n",
      "Date 15/10/2024 is in df_x but not in ASQ_merged\n",
      "Date 30/10/2024 is in df_x but not in ASQ_merged\n",
      "Date 29/11/2024 is in df_x but not in ASQ_merged\n",
      "Date 05/12/2024 is in ASQ_merged but not in df_x\n",
      "Date 03/12/2024 is in ASQ_merged but not in df_x\n",
      "Date 04/10/2024 is in df_x but not in ASQ_merged\n",
      "Date 09/10/2024 is in df_x but not in ASQ_merged\n",
      "Date 04/04/2025 is in df_x but not in ASQ_merged\n",
      "Date 11/11/2024 is in df_x but not in ASQ_merged\n",
      "Date 25/11/2024 is in df_x but not in ASQ_merged\n",
      "Date 17/10/2024 is in df_x but not in ASQ_merged\n",
      "Date 05/11/2024 is in df_x but not in ASQ_merged\n",
      "Date 21/10/2024 is in df_x but not in ASQ_merged\n",
      "Date 25/10/2024 is in df_x but not in ASQ_merged\n",
      "Date 07/10/2024 is in df_x but not in ASQ_merged\n",
      "Date 17/12/2024 is in ASQ_merged but not in df_x\n",
      "Date 10/10/2024 is in df_x but not in ASQ_merged\n",
      "Date 22/10/2024 is in df_x but not in ASQ_merged\n"
     ]
    }
   ],
   "source": [
    "df_x = df[df['Outlet'] == 'Asia Square'].copy()\n",
    "\n",
    "\n",
    "df_x['Date'] = pd.to_datetime(df_x['Date'], format='%d/%m/%Y', errors='coerce', dayfirst=True)\n",
    "ASQ_merged['date'] = pd.to_datetime(ASQ_merged['date'], format='%d/%m/%Y', errors='coerce', dayfirst=True)\n",
    "\n",
    "df_x['Date'] = df_x['Date'].dt.strftime('%d/%m/%Y')\n",
    "ASQ_merged['date'] = ASQ_merged['date'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "print(df_x['Date'].head())\n",
    "print(ASQ_merged['date'].head())\n",
    "\n",
    "df_x_dates = set(df_x['Date'].unique())\n",
    "ASQ_merged_dates = set(ASQ_merged['date'].unique())\n",
    "\n",
    "# Find dates that are in one dataframe but not the other\n",
    "mismatched_dates = df_x_dates.symmetric_difference(ASQ_merged_dates)\n",
    "\n",
    "print(f\"Number of mismatched dates: {len(mismatched_dates)}\")\n",
    "\n",
    "for date in mismatched_dates:\n",
    "    if date in df_x_dates:\n",
    "        print(f\"Date {date} is in df_x but not in ASQ_merged\")\n",
    "    else:\n",
    "        print(f\"Date {date} is in ASQ_merged but not in df_x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "880fd85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 6)\n",
      "(121, 6)\n",
      "(121, 6)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ASQ_merged_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(RC_merged\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(KATONG_merged\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mASQ_merged_full\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(RC_merged_full\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(KATONG_merged_full\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ASQ_merged_full' is not defined"
     ]
    }
   ],
   "source": [
    "print(ASQ_merged.shape)\n",
    "print(RC_merged.shape)\n",
    "print(KATONG_merged.shape)\n",
    "\n",
    "print(ASQ_merged_full.shape)\n",
    "print(RC_merged_full.shape)\n",
    "print(KATONG_merged_full.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7abf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RC_merged_full"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
