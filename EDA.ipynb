{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dd8a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db7cd953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define food and drinks sets\n",
    "food = {\n",
    "    'Add On', 'Bar Snack', 'Bites', 'Brunch', 'Burger', 'Chargrill', 'Chicken or Prawn Add ',\n",
    "    'Combo', 'Dessert', 'Doneness', 'Foods', 'GF Add Ons', 'GF Burger and Hotdog', 'Mains',\n",
    "    'Pasta', 'Pizza', 'Platter', 'Serving Choice', 'Sides', 'Smoothie', 'Soup', 'Starter', 'Hotdog'\n",
    "}\n",
    "\n",
    "drinks = {\n",
    "    'Bottled Beer', 'Bourbon', 'Choice of Margarita / ', 'Ciders', 'Cocktail', 'Coffee',\n",
    "    'Coffee Options', 'Cognac', 'Corkage', 'Draught Beers', 'Drinks', 'Gin', 'Juices',\n",
    "    'Liqueurs & Aperitifs', 'Live Craft Beer', 'Mineral Water', 'Mocktails', 'Red Wine',\n",
    "    'Rose Wine', 'Rum', 'Soft Drink', 'Sparkling & Champagne', 'Tea', 'Tequila', 'Vodka',\n",
    "    'Whisky', 'White Wine'\n",
    "}\n",
    "\n",
    "# Date range boundaries\n",
    "start_date = pd.Timestamp('2024-12-01')\n",
    "end_date = pd.Timestamp('2025-03-31')\n",
    "\n",
    "def clean_itemgroup_data(file_path):\n",
    "    # Read Excel file\n",
    "    df = pd.read_excel(file_path, engine='xlrd', skiprows=4)\n",
    "    df = df[:-1]\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    df.columns = ['Date'] + list(df.columns[1:])\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    if 'Unnamed: 2' in df.columns:\n",
    "        df = df.drop(columns=['Unnamed: 2'])\n",
    "    \n",
    "    df = df[df['Date'] != 'Total']\n",
    "    df = df[df['Date'].notna()]\n",
    "    \n",
    "    # Drop columns with 'Unnamed' prefix\n",
    "    unnamed_columns = [col for col in df.columns if col.startswith('Unnamed')]\n",
    "    df = df.drop(columns=unnamed_columns)\n",
    "    \n",
    "    # Format the Date column as Timestamp\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%d-%b-%y')\n",
    "    \n",
    "    # Filter by date range (using Timestamps, not string comparison)\n",
    "    df = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]\n",
    "    \n",
    "    # (Removed the line converting Date back to string to keep it as Timestamp)\n",
    "    \n",
    "    # Convert all columns except 'Date' to float, handling non-numeric values\n",
    "    for col in df.columns[1:]:\n",
    "        df[col] = pd.to_numeric(df[col], errors='raise')\n",
    "    \n",
    "    # Calculate total_food and total_drinks for each row\n",
    "    food_cols = list(food.intersection(df.columns))\n",
    "    drink_cols = list(drinks.intersection(df.columns))\n",
    "\n",
    "    df['total_food'] = df[food_cols].sum(axis=1, skipna=True)\n",
    "    df['total_drinks'] = df[drink_cols].sum(axis=1, skipna=True)\n",
    "    \n",
    "    # Drop all columns except Date, total_food, total_drinks, Total\n",
    "    columns_to_keep = ['Date', 'total_food', 'total_drinks', 'Total']\n",
    "    df = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "    \n",
    "    # Convert column names to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_daily_summary(file_path):\n",
    "    # Read CSV file with appropriate skipping of rows and footers\n",
    "    df = pd.read_csv(file_path, skiprows=4, skipfooter=7, engine='python')\n",
    "    \n",
    "    # Drop unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if col.startswith('Unnamed:')]\n",
    "    df.drop(columns=unnamed_columns, inplace=True)\n",
    "    \n",
    "    # Convert START column to date (Timestamp) format\n",
    "    df['START'] = pd.to_datetime(df['START'], format='%d/%m/%Y %H:%M:%S', errors='raise')\n",
    "    \n",
    "    # Filter by date range (Timestamp comparison)\n",
    "    df = df[(df['START'] >= start_date) & (df['START'] <= end_date)]\n",
    "    \n",
    "    # (Removed the line converting START to string to keep it as Timestamp)\n",
    "    \n",
    "    # Drop all other columns except START, ORDERS COUNT, AVG ORDER\n",
    "    columns_to_keep = ['START', 'ORDERS COUNT', 'AVG ORDER']\n",
    "    df = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "    \n",
    "    # Rename START to date\n",
    "    df.rename(columns={'START': 'date'}, inplace=True)\n",
    "    \n",
    "    # Convert all column names to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    # Group by date and sum the values for duplicates\n",
    "    df = df.groupby('date', as_index=False).sum()\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_outlet(itemgroup, daily_summary, outlet_name):\n",
    "    # Convert the 'date' columns to date only (discarding any time info)\n",
    "    itemgroup['date'] = pd.to_datetime(itemgroup['date'], errors='raise').dt.date\n",
    "    daily_summary['date'] = pd.to_datetime(daily_summary['date'], errors='raise').dt.date\n",
    "\n",
    "    # Merge itemgroup and daily summary using an outer join on date (now date-only)\n",
    "    merged = pd.merge(itemgroup, daily_summary, on='date', how='outer', suffixes=('_itemgroup', '_daily'))\n",
    "    \n",
    "    # Sort by date\n",
    "    merged = merged.sort_values(by='date').reset_index(drop=True)\n",
    "    \n",
    "    # Rename the merged DataFrame for each outlet\n",
    "    merged.name = f\"{outlet_name}_merged\"\n",
    "    return merged\n",
    "\n",
    "def print_mismatched_dates(df_full, merged_df, outlet_name):\n",
    "    # Filter df_full to the target outlet and copy\n",
    "    df_outlet = df_full[df_full['Outlet'] == outlet_name].copy()\n",
    "    \n",
    "    # Convert both sets of Date columns to datetime\n",
    "    df_outlet['Date'] = pd.to_datetime(df_outlet['Date'], errors='raise')\n",
    "    merged_df['date'] = pd.to_datetime(merged_df['date'], errors='raise')\n",
    "    \n",
    "    # Reduce to date-only\n",
    "    df_outlet_date_only = df_outlet['Date'].dt.date\n",
    "    merged_outlet_date_only = merged_df['date'].dt.date\n",
    "    \n",
    "    # Build sets of unique date-only values\n",
    "    df_dates = set(df_outlet_date_only.unique())\n",
    "    merged_dates = set(merged_outlet_date_only.unique())\n",
    "    \n",
    "    # Find mismatched dates\n",
    "    mismatched_dates = df_dates.symmetric_difference(merged_dates)\n",
    "    \n",
    "    print(f\"Outlet: {outlet_name}\")\n",
    "    print(f\"Forgotten Dates:\")\n",
    "    for date in mismatched_dates:\n",
    "        if date in df_dates:\n",
    "            print(f\"  - {date} is in df but not in merged\")\n",
    "        else:\n",
    "            print(f\"  - {date}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "def check_duplicates_in_df(df, outlet_name):\n",
    "    \"\"\"\n",
    "    Checks for any duplicate dates (ignoring time) in df for a given outlet.\n",
    "    \"\"\"\n",
    "    # 1) Convert the 'Date' column to datetime if not already\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='raise')\n",
    "    \n",
    "    # 2) Create a date_only column\n",
    "    df['date'] = df['Date'].dt.date\n",
    "    \n",
    "    # 3) Group by date_only and see if any date appears more than once\n",
    "    duplicates = (\n",
    "        df.groupby('date')\n",
    "          .size()\n",
    "          .reset_index(name='count')\n",
    "          .query('count > 1')\n",
    "    )\n",
    "    \n",
    "    # 4) Report results\n",
    "    print(f\"==== Duplicates for {outlet_name} ====\")\n",
    "    if duplicates.empty:\n",
    "        print(\"No duplicate dates found.\")\n",
    "    else:\n",
    "        print(duplicates)\n",
    "    print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "def merge_on_date_only(\n",
    "    df_outlet,       # the smaller subset DataFrame\n",
    "    df_merged,       # the superset merged DataFrame\n",
    "    date_col_outlet='Date',\n",
    "    date_col_merged='date',\n",
    "    how='left'\n",
    "):\n",
    "    \"\"\"\n",
    "    Merge df_outlet with df_merged on date only (ignoring time).\n",
    "    By default, keeps all rows in df_merged (the 'superset').\n",
    "    \"\"\"\n",
    "    # Convert to datetime\n",
    "    df_outlet[date_col_outlet] = pd.to_datetime(df_outlet[date_col_outlet])\n",
    "    df_merged[date_col_merged] = pd.to_datetime(df_merged[date_col_merged])\n",
    "    \n",
    "    # Extract date only\n",
    "    df_outlet['date_only'] = df_outlet[date_col_outlet].dt.date\n",
    "    df_merged['date_only'] = df_merged[date_col_merged].dt.date\n",
    "    \n",
    "    # Merge on the date_only column\n",
    "    merged_final = pd.merge(\n",
    "        df_merged,\n",
    "        df_outlet,\n",
    "        on='date_only',\n",
    "        how=how\n",
    "    )\n",
    "    \n",
    "    return merged_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0afc340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'daily_data\\Picotin_overview.xlsx', engine='openpyxl')\n",
    "df = df.drop(columns=['Timestamp'])\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# Manually replace incorrect entries\n",
    "df.loc[167, 'Date'] = '2024-12-11 00:00:00'\n",
    "df.loc[170, 'Date'] = '2024-12-12 00:00:00'\n",
    "df.loc[456, 'Date'] = '2025-04-01 00:00:00'\n",
    "df.loc[459, 'Date'] = '2025-04-02 00:00:00'\n",
    "df.loc[461, 'Date'] = '2025-04-03 00:00:00'\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='raise')\n",
    "\n",
    "# Removed string formatting so we keep Timestamps\n",
    "# df['Date'] = df['Date'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "# Filter with actual Timestamps\n",
    "df = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)].copy()\n",
    "\n",
    "ASQ_itemgroup = clean_itemgroup_data(r'daily_data\\ASQ_itemgroup_daily.xls')\n",
    "ASQ_daily_summary = clean_daily_summary(r'daily_data\\ASQ_daily_summary.csv')\n",
    "\n",
    "KATONG_itemgroup = clean_itemgroup_data(r'daily_data\\KATONG_itemgroup_daily.xls')\n",
    "KATONG_daily_summary = clean_daily_summary(r'daily_data\\KATONG_daily_summary.csv')\n",
    "\n",
    "RC_itemgroup = clean_itemgroup_data(r'daily_data\\RC_itemgroup_daily.xls')\n",
    "RC_daily_summary = clean_daily_summary(r'daily_data\\RC_daily_summary.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccfc2b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outlet: ASQ\n",
      "Dates missing in itemgroup: []\n",
      "Dates missing in daily summary: []\n",
      "Duplicate dates in daily summary: []\n",
      "Duplicate dates in itemgroup: []\n",
      "\n",
      "Outlet: KATONG\n",
      "Dates missing in itemgroup: []\n",
      "Dates missing in daily summary: [datetime.date(2025, 3, 31)]\n",
      "Duplicate dates in daily summary: []\n",
      "Duplicate dates in itemgroup: []\n",
      "\n",
      "Outlet: RC\n",
      "Dates missing in itemgroup: []\n",
      "Dates missing in daily summary: [datetime.date(2024, 12, 1), datetime.date(2024, 12, 5), datetime.date(2024, 12, 24), datetime.date(2025, 1, 18), datetime.date(2025, 3, 31)]\n",
      "Duplicate dates in daily summary: [datetime.date(2024, 12, 4) datetime.date(2024, 12, 23)\n",
      " datetime.date(2025, 1, 17)]\n",
      "Duplicate dates in itemgroup: []\n"
     ]
    }
   ],
   "source": [
    "outlets = ['ASQ', 'KATONG', 'RC']\n",
    "\n",
    "for outlet in outlets:\n",
    "    # Access the respective DataFrames dynamically\n",
    "    daily_summary = globals().get(f'{outlet}_daily_summary')\n",
    "    itemgroup = globals().get(f'{outlet}_itemgroup')\n",
    "    \n",
    "    if daily_summary is None or itemgroup is None:\n",
    "        print(f\"\\nWarning: DataFrames for {outlet} are not loaded.\")\n",
    "        continue\n",
    "\n",
    "    # Convert Timestamps to *date* only (ignore time)\n",
    "    summary_date_only = daily_summary['date'].dropna().dt.date\n",
    "    itemgroup_date_only = itemgroup['date'].dropna().dt.date\n",
    "\n",
    "    # Find unique dates (no time)\n",
    "    summary_dates = set(summary_date_only.unique())\n",
    "    itemgroup_dates = set(itemgroup_date_only.unique())\n",
    "\n",
    "    # Find missing dates, ignoring time\n",
    "    missing_in_itemgroup = summary_dates - itemgroup_dates\n",
    "    missing_in_summary   = itemgroup_dates - summary_dates\n",
    "\n",
    "    # Find duplicate dates, ignoring time\n",
    "    duplicates_in_summary    = summary_date_only[summary_date_only.duplicated()]\n",
    "    duplicates_in_itemgroup  = itemgroup_date_only[itemgroup_date_only.duplicated()]\n",
    "\n",
    "    # Display the results for the current outlet\n",
    "    print(f\"\\nOutlet: {outlet}\")\n",
    "    print(\"Dates missing in itemgroup:\", sorted(missing_in_itemgroup))\n",
    "    print(\"Dates missing in daily summary:\", sorted(missing_in_summary))\n",
    "    print(\"Duplicate dates in daily summary:\", duplicates_in_summary.unique())\n",
    "    print(\"Duplicate dates in itemgroup:\", duplicates_in_itemgroup.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75a73fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASQ_merged = merge_outlet(ASQ_itemgroup, ASQ_daily_summary, 'ASQ')\n",
    "KATONG_merged = merge_outlet(KATONG_itemgroup, KATONG_daily_summary, 'KATONG')\n",
    "RC_merged = merge_outlet(RC_itemgroup, RC_daily_summary, 'RC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d5591e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of 'Date' in df:\n",
      "datetime64[ns]\n",
      "\n",
      "Data type of 'date' in ASQ_merged:\n",
      "object\n",
      "\n",
      "Number of empty or NaN values in 'Date' column of df:\n",
      "0\n",
      "\n",
      "Number of empty or NaN values in 'date' column of ASQ_merged:\n",
      "0\n",
      "\n",
      "Rows with empty 'Date' in df:\n",
      "Empty DataFrame\n",
      "Columns: [Outlet, Date, Day, Breakfast Sales, Lunch Sales, Evening Sales, Dinner Lunch Sales, Night Sales, Total Sales, Number of Breakfast Floor Staff, Number of Lunch Floor Staff, Number of Evening Floor Staff, Number of Dinner Floor Staff, Number of Night Floor Staff, Number of Breakfast Kitchen Staff, Number of Lunch Kitchen Staff, Number of Evening Kitchen Staff, Number of Dinner Kitchen Staff, Number of Night Kitchen Staff, Total Taxi Claims, Last Bill Closed, Closing Time, Closing Manager Name, Total Floor Staff Hours, Total Kitchen Staff Hours]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 25 columns]\n",
      "\n",
      "Rows with empty 'date' in ASQ_merged:\n",
      "Empty DataFrame\n",
      "Columns: [date, total_food, total_drinks, total, orders count, avg order]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df_copy = df.copy()\n",
    "\n",
    "# Print data types of both date columns\n",
    "print(\"Data type of 'Date' in df:\")\n",
    "print(df['Date'].dtype)\n",
    "print(\"\\nData type of 'date' in ASQ_merged:\")\n",
    "print(ASQ_merged['date'].dtype)\n",
    "\n",
    "# Check for empty or null values in both date columns\n",
    "print(\"\\nNumber of empty or NaN values in 'Date' column of df:\")\n",
    "print(df['Date'].isna().sum())\n",
    "\n",
    "print(\"\\nNumber of empty or NaN values in 'date' column of ASQ_merged:\")\n",
    "print(ASQ_merged['date'].isna().sum())\n",
    "\n",
    "# Print rows where Date is NaN in both dataframes\n",
    "print(\"\\nRows with empty 'Date' in df:\")\n",
    "print(df[df['Date'].isna()])\n",
    "\n",
    "print(\"\\nRows with empty 'date' in ASQ_merged:\")\n",
    "print(ASQ_merged[ASQ_merged['date'].isna()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa96e22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlet: Asia Square\n",
      "Forgotten Dates:\n",
      "  - 2024-12-05\n",
      "  - 2024-12-31\n",
      "  - 2024-12-02\n",
      "  - 2024-12-17\n",
      "  - 2024-12-03\n",
      "  - 2024-12-04\n",
      "--------------------------------------------------\n",
      "Outlet: Katong\n",
      "Forgotten Dates:\n",
      "  - 2024-12-09\n",
      "  - 2024-12-08\n",
      "  - 2024-12-05\n",
      "  - 2024-12-28\n",
      "  - 2024-12-13\n",
      "  - 2024-12-02\n",
      "  - 2024-12-17\n",
      "  - 2024-12-01\n",
      "  - 2024-12-11\n",
      "  - 2024-12-03\n",
      "  - 2024-12-18\n",
      "  - 2024-12-06\n",
      "  - 2024-12-07\n",
      "  - 2024-12-04\n",
      "--------------------------------------------------\n",
      "Outlet: Rochester\n",
      "Forgotten Dates:\n",
      "  - 2024-12-09\n",
      "  - 2024-12-08\n",
      "  - 2024-12-05\n",
      "  - 2024-12-22\n",
      "  - 2025-02-09\n",
      "  - 2024-12-31\n",
      "  - 2025-01-28\n",
      "  - 2024-12-29\n",
      "  - 2024-12-15\n",
      "  - 2024-12-02\n",
      "  - 2025-03-31\n",
      "  - 2024-12-06\n",
      "  - 2024-12-07\n",
      "  - 2024-12-04\n",
      "  - 2024-12-10\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_mismatched_dates(df, ASQ_merged, \"Asia Square\")\n",
    "print_mismatched_dates(df, KATONG_merged, \"Katong\")\n",
    "print_mismatched_dates(df, RC_merged, \"Rochester\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "964a6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_KATONG = df[df['Outlet'] == 'Katong'].copy() \n",
    "df_ASQ = df[df['Outlet'] == 'Asia Square'].copy()\n",
    "df_RC = df[df['Outlet'] == 'Rochester'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "106b9c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Duplicates for KATONG ====\n",
      "          date  count\n",
      "14  2024-12-29      2\n",
      "--------------------------------------------------\n",
      "\n",
      "==== Duplicates for ASQ ====\n",
      "         date  count\n",
      "6  2024-12-16      2\n",
      "--------------------------------------------------\n",
      "\n",
      "==== Duplicates for RC ====\n",
      "          date  count\n",
      "3   2024-12-12      2\n",
      "77  2025-03-02      2\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_duplicates_in_df(df_KATONG, 'KATONG')\n",
    "check_duplicates_in_df(df_ASQ, 'ASQ')\n",
    "check_duplicates_in_df(df_RC, 'RC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7ff19ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge for Katong\n",
    "KATONG_final = merge_on_date_only(\n",
    "    df_outlet=df_KATONG,\n",
    "    df_merged=KATONG_merged,\n",
    "    date_col_outlet='Date',   # col in df_KATONG\n",
    "    date_col_merged='date',   # col in KATONG_merged\n",
    "    how='left'                # keep all rows from KATONG_merged\n",
    ")\n",
    "\n",
    "# Merge for Asia Square\n",
    "ASQ_final = merge_on_date_only(\n",
    "    df_outlet=df_ASQ,\n",
    "    df_merged=ASQ_merged,\n",
    "    date_col_outlet='Date',\n",
    "    date_col_merged='date',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge for Rochester\n",
    "RC_final = merge_on_date_only(\n",
    "    df_outlet=df_RC,\n",
    "    df_merged=RC_merged,\n",
    "    date_col_outlet='Date',\n",
    "    date_col_merged='date',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Combine all three into one DataFrame\n",
    "combined = pd.concat([KATONG_final, ASQ_final, RC_final], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f58dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_excel('picotin_dec_march.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
